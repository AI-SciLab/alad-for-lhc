{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import importlib.util\n",
    "\n",
    "import matplotlib\n",
    "# matplotlib.rcParams['text.usetex'] = True\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "\n",
    "from alad_mod.alad import ALAD\n",
    "from evaluation.histogram_builder import *\n",
    "from data.hlf_preprocessing import load"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "loading alad\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\nW1116 15:19:14.683005 139955663333184 lazy_loader.py:50] \nThe TensorFlow contrib module will not be included in TensorFlow 2.0.\nFor more information, please see:\n  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n  * https://github.com/tensorflow/addons\n  * https://github.com/tensorflow/io (for I/O related ops)\nIf you depend on functionality not listed there, please file an issue.\n\n",
      "W1116 15:19:14.729100 139955663333184 deprecation_wrapper.py:119] From /home/oliverkn/cloud/eth/2019_FS/pro/pycharm/alad_mod/alad.py:29: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n\n",
      "W1116 15:19:14.732089 139955663333184 deprecation_wrapper.py:119] From /home/oliverkn/cloud/eth/2019_FS/pro/pycharm/alad_mod/alad.py:44: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n\n",
      "W1116 15:19:14.732914 139955663333184 deprecation.py:323] From /home/oliverkn/pro/alad_6021/2_l16/config.py:81: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse keras.layers.dense instead.\n",
      "W1116 15:19:15.126356 139955663333184 deprecation.py:323] From /home/oliverkn/pro/alad_6021/2_l16/config.py:168: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
      "W1116 15:19:15.241086 139955663333184 deprecation.py:323] From /home/oliverkn/pro/alad_6021/2_l16/config.py:176: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse keras.layers.dropout instead.\n",
      "W1116 15:19:15.580147 139955663333184 deprecation.py:323] From /home/oliverkn/pro/pycharm/venv/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W1116 15:19:15.659983 139955663333184 deprecation_wrapper.py:119] From /home/oliverkn/cloud/eth/2019_FS/pro/pycharm/alad_mod/alad.py:126: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n\n",
      "W1116 15:19:15.660875 139955663333184 deprecation_wrapper.py:119] From /home/oliverkn/cloud/eth/2019_FS/pro/pycharm/alad_mod/alad.py:133: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n\n",
      "W1116 15:19:15.661475 139955663333184 deprecation_wrapper.py:119] From /home/oliverkn/cloud/eth/2019_FS/pro/pycharm/alad_mod/alad.py:133: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n\n",
      "W1116 15:19:15.662462 139955663333184 deprecation_wrapper.py:119] From /home/oliverkn/cloud/eth/2019_FS/pro/pycharm/alad_mod/alad.py:143: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n\n",
      "W1116 15:19:17.881765 139955663333184 deprecation.py:323] From /home/oliverkn/pro/pycharm/venv/lib/python3.6/site-packages/tensorflow/python/training/moving_averages.py:433: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "W1116 15:19:18.543254 139955663333184 deprecation.py:323] From /home/oliverkn/pro/pycharm/venv/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse keras.layers.flatten instead.\n",
      "W1116 15:19:18.745633 139955663333184 deprecation.py:506] From /home/oliverkn/cloud/eth/2019_FS/pro/pycharm/alad_mod/alad.py:214: calling norm (from tensorflow.python.ops.linalg_ops) with keep_dims is deprecated and will be removed in a future version.\nInstructions for updating:\nkeep_dims is deprecated, use keepdims instead\n",
      "W1116 15:19:18.779260 139955663333184 deprecation_wrapper.py:119] From /home/oliverkn/cloud/eth/2019_FS/pro/pycharm/alad_mod/alad.py:429: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n\n",
      "W1116 15:19:18.874813 139955663333184 deprecation.py:323] From /home/oliverkn/pro/pycharm/venv/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse standard file APIs to check for files with this prefix.\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "result_path = '/home/oliverkn/pro/alad_6021/2_l16'\n",
    "model_file = 'model-10000000'\n",
    "\n",
    "print('loading alad')\n",
    "\n",
    "# loading config\n",
    "spec = importlib.util.spec_from_file_location('config', os.path.join(result_path, 'config.py'))\n",
    "config_alad = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(config_alad)\n",
    "\n",
    "# loading preprocessor\n",
    "preprocessor = load(os.path.join(result_path, 'preprocessor.pkl'))\n",
    "\n",
    "# loading alad\n",
    "tf.reset_default_graph()\n",
    "ad = ALAD(config_alad, tf.Session())\n",
    "ad.load(os.path.join(result_path, model_file))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Load ALAD\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "lum = 4429\n",
    "\n",
    "sets = {}\n",
    "sets['dy1jets'] = {'file': '/home/oliverkn/pro/opendata_v2/7719/data.hdf5', 'xsec':561, 'K':1.23}\n",
    "sets['dy2jets'] = {'file': '/home/oliverkn/pro/opendata_v2/7721/data.hdf5', 'xsec':181, 'K':1.23}\n",
    "sets['dy3jets'] = {'file': '/home/oliverkn/pro/opendata_v2/7722/data.hdf5', 'xsec':51, 'K':1.23}\n",
    "sets['dy4jets'] = {'file': '/home/oliverkn/pro/opendata_v2/7723/data.hdf5', 'xsec':15, 'K':1.23}\n",
    "\n",
    "sets['w1jets'] = {'file': '/home/oliverkn/pro/opendata_v2/9863/data.hdf5', 'xsec':4480, 'K':1.23}\n",
    "sets['w2jets'] = {'file': '/home/oliverkn/pro/opendata_v2/9864/data.hdf5', 'xsec':1435, 'K':1.23}\n",
    "sets['w3jets'] = {'file': '/home/oliverkn/pro/opendata_v2/9865/data.hdf5', 'xsec':304, 'K':1.23}\n",
    "\n",
    "# sets['dy1jets'] = {'file': '/home/oliverkn/pro/opendata_v2/7730/data.hdf5', 'xsec':1141, 'K':1}\n",
    "# sets['dy2jets'] = {'file': '/home/oliverkn/pro/opendata_v2/7723/data.hdf5', 'xsec':0, 'K':1.23}\n",
    "# sets['dy3jets'] = {'file': '/home/oliverkn/pro/opendata_v2/7723/data.hdf5', 'xsec':0, 'K':1.23}\n",
    "# sets['dy4jets'] = {'file': '/home/oliverkn/pro/opendata_v2/7723/data.hdf5', 'xsec':0, 'K':1.23}\n",
    "# \n",
    "# sets['w1jets'] = {'file': '/home/oliverkn/pro/opendata_v2/9938/data.hdf5', 'xsec':5090, 'K':1}\n",
    "# sets['w2jets'] = {'file': '/home/oliverkn/pro/opendata_v2/9940/data.hdf5', 'xsec':7110, 'K':1}\n",
    "# sets['w3jets'] = {'file': '/home/oliverkn/pro/opendata_v2/9865/data.hdf5', 'xsec':0, 'K':1}\n",
    "\n",
    "sets['ttbar'] = {'file': '/home/oliverkn/pro/opendata_v2/9588/data.hdf5', 'xsec':164, 'K':1.66}\n",
    "\n",
    "sets['data'] = {'file': '/home/oliverkn/pro/opendata_v2/6021/data.hdf5', 'xsec':1, 'K':1}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "loading dy1jets\n3056142\n",
      "loading dy2jets\n986028\nloading dy3jets\n277831\nloading dy4jets\n81715\nloading w1jets\n24405561\nloading w2jets\n7817406\n",
      "loading w3jets\n1656091\nloading ttbar\n1205750\nloading data\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "data_dict = {}\n",
    "\n",
    "def pre_select(x):\n",
    "    filter_iso = x[:,7] + x[:,8] + x[:,9] < 0.1\n",
    "    filter_eta = np.abs(x[:,5]) < 1.4\n",
    "    # filter_eta2 = np.logical_or(np.abs(x[:,5]) < 1.44, np.abs(x[:,5]) > 1.56)\n",
    "    filter_njets = x[:,2] > 1\n",
    "    # filter_idx = filter_njets * filter_eta * filter_iso #* filter_eta2\n",
    "    filter_idx = filter_iso * filter_eta * filter_njets\n",
    "    \n",
    "    return x[filter_idx]\n",
    "\n",
    "for key, set in sets.items():\n",
    "    print('loading ' + key)\n",
    "\n",
    "    file = set['file']\n",
    "    hdf5_file = h5py.File(file, 'r')\n",
    "    \n",
    "    data = hdf5_file['data']\n",
    "    n_tot = hdf5_file['n_tot'][()]\n",
    "    \n",
    "    if key == 'data':\n",
    "        x = data\n",
    "    else:\n",
    "        \n",
    "        n_tot_target = int(lum * set['xsec'] * set['K'])\n",
    "        print(n_tot_target)\n",
    "        x = data[0:n_tot_target]\n",
    "    \n",
    "    # run preselection\n",
    "    #x = pre_select(x)\n",
    "    \n",
    "    set['x'] = x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Load data into memory\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "dy1jets: (907244, 23)\ndy2jets: (308661, 23)\ndy3jets: (277831, 23)\ndy4jets: (81715, 23)\nw1jets: (3413855, 23)\nw2jets: (1667510, 23)\nw3jets: (361756, 23)\nttbar: (1205750, 23)\ndata: (38247438, 23)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "for key, set in sets.items():\n",
    "    print('%s: %s' % (key, set['x'].shape))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "score_type = 'l1'\n",
    "\n",
    "cont_bins = 40\n",
    "hist_settings = {}\n",
    "hist_settings['HT'] = {'symbol': '$H_T$ [GeV]', 'range': (0, 3000), 'yscale': 'log', 'bins': cont_bins, 'int': False}\n",
    "hist_settings['mass_jet'] = {'symbol': '$M_J$ [GeV]', 'range': (0, 3000), 'yscale': 'log', 'bins': cont_bins, 'int': False}\n",
    "hist_settings['n_jet'] = {'symbol': '$N_J$', 'range': (0, 15), 'yscale': 'linear', 'int': True, 'bin_size': 1}\n",
    "hist_settings['n_bjet'] = {'symbol': '$N_b$', 'range': (0, 15), 'yscale': 'linear', 'int': True, 'bin_size': 1}\n",
    "hist_settings['lep_pt'] = {'range': (20, 1000), 'yscale': 'log', 'bins': cont_bins, 'int': False}\n",
    "hist_settings['lep_eta'] = {'range': (-2.5, 2.5), 'yscale': 'linear', 'bins': cont_bins, 'int': False}\n",
    "hist_settings['lep_charge'] = {'range': (-1, 1), 'yscale': 'linear', 'int': True}\n",
    "hist_settings['lep_iso_ch'] = {'range': (0, 0.1), 'yscale': 'log', 'bins': cont_bins, 'int': False}\n",
    "hist_settings['lep_iso_neu'] = {'range': (0, 0.1), 'yscale': 'log', 'bins': cont_bins, 'int': False}\n",
    "hist_settings['lep_iso_gamma'] = {'range': (0, 0.1), 'yscale': 'log', 'bins': cont_bins, 'int': False}\n",
    "hist_settings['MET'] = {'range': (0, 1000), 'yscale': 'log', 'bins': cont_bins, 'int': False}\n",
    "hist_settings['METo'] = {'range': (-100, 100), 'yscale': 'linear', 'bins': cont_bins, 'int': False}\n",
    "hist_settings['METp'] = {'range': (-100, 100), 'yscale': 'linear', 'bins': cont_bins, 'int': False}\n",
    "hist_settings['MT'] = {'range': (0, 200), 'yscale': 'log', 'bins': cont_bins, 'int': False}\n",
    "hist_settings['n_mu'] = {'range': (0, 15), 'yscale': 'linear', 'int': True, 'bin_size': 1}\n",
    "hist_settings['pt_mu'] = {'range': (0, 1000), 'yscale': 'log', 'bins': cont_bins, 'int': False}\n",
    "hist_settings['mass_mu'] = {'range': (0, 1000), 'yscale': 'log', 'bins': cont_bins, 'int': False}\n",
    "hist_settings['n_ele'] = {'range': (0, 15), 'yscale': 'linear', 'int': True, 'bin_size': 1}\n",
    "hist_settings['pt_ele'] = {'range': (0, 1000), 'yscale': 'log', 'bins': cont_bins, 'int': False}\n",
    "hist_settings['mass_ele'] = {'range': (0, 1000), 'yscale': 'log', 'bins': cont_bins, 'int': False}\n",
    "hist_settings['n_neu'] = {'range': (0, 400), 'yscale': 'linear', 'int': True, 'bin_size': 1}\n",
    "hist_settings['n_ch'] = {'range': (0, 1000), 'yscale': 'linear', 'int': True, 'bin_size': 1}\n",
    "hist_settings['n_photon'] = {'range': (0, 1000), 'yscale': 'linear', 'int': True, 'bin_size': 1}\n",
    "\n",
    "def pre_select(x):\n",
    "    filter_iso = x[:,7] + x[:,8] + x[:,9] < 0.1\n",
    "    filter_eta = np.abs(x[:,5]) < 1.4\n",
    "    # filter_eta2 = np.logical_or(np.abs(x[:,5]) < 1.44, np.abs(x[:,5]) > 1.56)\n",
    "    filter_njets = x[:,2] > 1\n",
    "    # filter_idx = filter_njets * filter_eta * filter_iso #* filter_eta2\n",
    "    filter_idx = filter_iso * filter_eta * filter_njets\n",
    "    \n",
    "    return x[filter_idx]\n",
    "\n",
    "def anomaly_select(x, thres = None, q = None):\n",
    "    x_transformed = preprocessor.transform(x)\n",
    "    scores = ad.get_anomaly_scores(x_transformed, type=score_type)\n",
    "    \n",
    "    if thres is None:\n",
    "        thres = np.quantile(scores, 1 - q)\n",
    "        \n",
    "    print(thres)\n",
    "        \n",
    "    anomaly_idx = scores > thres\n",
    "    return x[anomaly_idx]\n",
    "\n",
    "def build_hists(x, selection_functions, n_max, batch_size=2 ** 20):\n",
    "    hist_builder_arr = [HistogramBuilder(hist_settings) for f in selection_functions]\n",
    "    \n",
    "    n = x.shape[0]\n",
    "    n = min(n, n_max)\n",
    "    n_batches = int(n / batch_size) + 1\n",
    "    \n",
    "    for t in range(n_batches):\n",
    "        print('batch number ' + str(t))\n",
    "        ran_from = int(t * batch_size)\n",
    "        ran_to = (t + 1) * batch_size\n",
    "        ran_to = int(np.clip(ran_to, 0, n))\n",
    "        \n",
    "        x_f = x[ran_from:ran_to]\n",
    "        for i, f in enumerate(selection_functions):\n",
    "            x_f = f(x_f)\n",
    "            hist_builder_arr[i].add_data(x_f)\n",
    "    \n",
    "    # perform upscaling\n",
    "    upscale_factor = x.shape[0] / n\n",
    "    print('processed/available: %d/%d = %f'%(n_events, x.shape[0], n/x.shape[0]))\n",
    "    hists = [scale_hists(builder.get_histogram_data(), upscale_factor) for builder in hist_builder_arr]\n",
    "    \n",
    "    return hists\n",
    "\n",
    "hist_cut_proc_dict = {}\n",
    "\n",
    "def build_set(set, selection_functions, n_max, batch_size=2 ** 20):\n",
    "     # load data\n",
    "    file = set['file']\n",
    "    h5file = h5py.File(file, 'r')\n",
    "    x = h5file['data']\n",
    "    \n",
    "    # build hists\n",
    "    hists = build_hists(x, selection_functions, thres=thres, q=None, n_max=n_max)\n",
    "    \n",
    "    # normalization\n",
    "    N_tot = h5file['n_tot'][()]\n",
    "    N_tot_target = lum * set['K'] * set['xsec']\n",
    "    weight =  N_tot_target / N_tot\n",
    "    print('N_tot=%d, N_tot_target=%d, w=%f'%(N_tot, N_tot_target, weight))\n",
    "    \n",
    "    for i in range(len(hists)):\n",
    "        hists[i] = scale_hists(hists[i], weight)\n",
    "    \n",
    "    # adding hists to dict\n",
    "    # for \n",
    "    # hist_cut_proc_dict[]\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "thres = 30\n",
    "n_max = int(1000e5)\n",
    "\n",
    "# data\n",
    "lum = 4429.0 #* 1.164090\n",
    "\n",
    "sets = {}\n",
    "sets['dy1jets'] = {'file': '/home/oliverkn/pro/opendata_v2/7719/data.hdf5', 'xsec':561, 'K':1.23}\n",
    "sets['dy2jets'] = {'file': '/home/oliverkn/pro/opendata_v2/7721/data.hdf5', 'xsec':181, 'K':1.23}\n",
    "sets['dy3jets'] = {'file': '/home/oliverkn/pro/opendata_v2/7722/data.hdf5', 'xsec':51, 'K':1.23}\n",
    "sets['dy4jets'] = {'file': '/home/oliverkn/pro/opendata_v2/7723/data.hdf5', 'xsec':15, 'K':1.23}\n",
    "\n",
    "sets['w1jets'] = {'file': '/home/oliverkn/pro/opendata_v2/9863/data.hdf5', 'xsec':4480, 'K':1.23}\n",
    "sets['w2jets'] = {'file': '/home/oliverkn/pro/opendata_v2/9864/data.hdf5', 'xsec':1435, 'K':1.23}\n",
    "sets['w3jets'] = {'file': '/home/oliverkn/pro/opendata_v2/9865/data.hdf5', 'xsec':304, 'K':1.23}\n",
    "\n",
    "# sets['dy1jets'] = {'file': '/home/oliverkn/pro/opendata_v2/7730/data.hdf5', 'xsec':1141, 'K':1}\n",
    "# sets['dy2jets'] = {'file': '/home/oliverkn/pro/opendata_v2/7723/data.hdf5', 'xsec':0, 'K':1.23}\n",
    "# sets['dy3jets'] = {'file': '/home/oliverkn/pro/opendata_v2/7723/data.hdf5', 'xsec':0, 'K':1.23}\n",
    "# sets['dy4jets'] = {'file': '/home/oliverkn/pro/opendata_v2/7723/data.hdf5', 'xsec':0, 'K':1.23}\n",
    "# \n",
    "# sets['w1jets'] = {'file': '/home/oliverkn/pro/opendata_v2/9938/data.hdf5', 'xsec':5090, 'K':1}\n",
    "# sets['w2jets'] = {'file': '/home/oliverkn/pro/opendata_v2/9940/data.hdf5', 'xsec':7110, 'K':1}\n",
    "# sets['w3jets'] = {'file': '/home/oliverkn/pro/opendata_v2/9865/data.hdf5', 'xsec':0, 'K':1}\n",
    "\n",
    "sets['ttbar'] = {'file': '/home/oliverkn/pro/opendata_v2/9588/data.hdf5', 'xsec':164, 'K':1.66}\n",
    "\n",
    "sets['data'] = {'file': '/home/oliverkn/pro/opendata_v2/6021/data.hdf5', 'xsec':1, 'K':1}\n",
    "\n",
    "\n",
    "for key, set in sets.items():\n",
    "    if key == 'data':\n",
    "        continue\n",
    "    \n",
    "    print('------------------------------building ' + key)\n",
    "    \n",
    "    # load data\n",
    "    file = set['file']\n",
    "    h5file = h5py.File(file, 'r')\n",
    "    x = h5file['data']\n",
    "    \n",
    "    if key == 'data':\n",
    "        N_tot = lum\n",
    "    else:\n",
    "        N_tot = h5file['n_tot'][()]\n",
    "    \n",
    "    # build hists\n",
    "    hist_raw, hist_pre, hist_ano, hist_pos = build_hists(x, thres=thres, q=None, n_max=n_max)\n",
    "    \n",
    "    n_events = hist_raw['HT'].n\n",
    "    n_events_a = hist_ano['HT'].n\n",
    "    print('%d / %d = %3d_ppm' % (n_events_a, n_events, (n_events_a / n_events*1e6)))\n",
    "    \n",
    "    # weight\n",
    "    fraction_processed = n_events/x.shape[0]\n",
    "    print('processed/available: %d/%d = %f'%(n_events, x.shape[0], fraction_processed))\n",
    "    \n",
    "    N_target = lum * set['K'] * set['xsec']\n",
    "    weight =  N_target / (N_tot * fraction_processed)\n",
    "    print('N_tot=%d, N_target=%d, w=%f'%(N_tot, N_target, weight))\n",
    "    \n",
    "    # scaling up hists\n",
    "    hist_raw = scale_hists(hist_raw, weight)\n",
    "    hist_pre = scale_hists(hist_pre, weight)\n",
    "    hist_ano = scale_hists(hist_ano, weight)\n",
    "    hist_pos = scale_hists(hist_pos, weight)\n",
    "    \n",
    "    set['hist_raw'] = hist_raw\n",
    "    set['hist_pre'] = hist_pre\n",
    "    set['hist_ano'] = hist_ano\n",
    "    set['hist_pos'] = hist_pos"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% compute hist\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sum_n_raw = 0\n",
    "sum_n_pre = 0\n",
    "sum_n_ano = 0\n",
    "for key in ['w1jets', 'w2jets', 'w3jets', 'dy1jets', 'dy2jets', 'dy3jets', 'dy4jets', 'ttbar']:\n",
    "    sum_n_raw += sets[key]['hist_raw']['HT'].n\n",
    "    sum_n_pre += sets[key]['hist_pre']['HT'].n\n",
    "    sum_n_ano += sets[key]['hist_ano']['HT'].n\n",
    "    \n",
    "for key in ['w1jets', 'w2jets', 'w3jets', 'dy1jets', 'dy2jets', 'dy3jets', 'dy4jets', 'ttbar']:\n",
    "    n = sets[key]['hist_raw']['HT'].n\n",
    "    print('%s: %d (%f)'%(key, n, n/sum_n_raw))\n",
    "\n",
    "print('--------------------------------------')\n",
    "print('n_tup (mc):   %d' % sum_n_raw)\n",
    "# print('n_tup (data): %d' % sets['data']['hist_raw']['HT'].n)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% confirm normalization\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "q_target = sum_n_ano / sum_n_pre\n",
    "print('Target ppm: %d' % (q_target * 1e6))\n",
    "\n",
    "# load data\n",
    "file = sets['data']['file']\n",
    "h5file = h5py.File(file, 'r')\n",
    "x = h5file['data']\n",
    "\n",
    "hist_raw, hist_pre, hist_ano, hist_pos = build_hists(x, thres=None, q=q_target, n_max=n_max)\n",
    "\n",
    "n_events = hist_pre['HT'].n\n",
    "n_events_a = hist_ano['HT'].n\n",
    "print('%d / %d = %3d_ppm' % (n_events_a, n_events, (n_events_a / n_events*1e6)))\n",
    "\n",
    "# weight\n",
    "fraction_processed = n_events/x.shape[0]\n",
    "print('processed/available: %d/%d = %f'%(n_events, x.shape[0], fraction_processed))\n",
    "\n",
    "weight =  1 / fraction_processed\n",
    "print('N_tot=%d, N_target=%d, w=%f'%(N_tot, N_target, weight))\n",
    "\n",
    "# scaling up hists\n",
    "hist_raw = scale_hists(hist_raw, weight)\n",
    "hist_pre = scale_hists(hist_pre, weight)\n",
    "hist_ano = scale_hists(hist_ano, weight)\n",
    "hist_pos = scale_hists(hist_pos, weight)\n",
    "\n",
    "set['hist_raw'] = hist_raw\n",
    "set['hist_pre'] = hist_pre\n",
    "set['hist_ano'] = hist_ano\n",
    "set['hist_pos'] = hist_pos"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% build data\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "W_proc_list = ['w1jets', 'w2jets', 'w3jets']\n",
    "DY_proc_list = ['dy1jets', 'dy2jets', 'dy3jets', 'dy4jets']\n",
    "ttbar_proc_list = ['ttbar']\n",
    "data_proc_list = ['data']\n",
    "\n",
    "b_proc_list = W_proc_list + DY_proc_list\n",
    "bs_proc_list = b_proc_list + ttbar_proc_list\n",
    "proc_list = bs_proc_list + data_proc_list\n",
    "\n",
    "# hist_cut_proc_dict[cut][proc]\n",
    "hist_cut_proc_dict = {}\n",
    "for cut in ['hist_raw', 'hist_pre', 'hist_ano','hist_pos']:\n",
    "    hist_proc_dict = {}\n",
    "    for proc in proc_list:\n",
    "        hist_proc_dict[proc] = sets[proc][cut]\n",
    "    hist_cut_proc_dict[cut]=hist_proc_dict\n",
    "    \n",
    "# sum sub processes W, DY, background, background + signal\n",
    "for cut in ['hist_raw', 'hist_pre', 'hist_ano', 'hist_pos']:\n",
    "    hist_proc_dict = hist_cut_proc_dict[cut]\n",
    "    \n",
    "    # sum W\n",
    "    hist_W = sum_hists([hist_proc_dict[proc] for proc in W_proc_list])\n",
    "    hist_proc_dict['W'] = hist_W\n",
    "    \n",
    "    # sum DY\n",
    "    hist_DY = sum_hists([hist_proc_dict[proc] for proc in DY_proc_list])\n",
    "    hist_proc_dict['DY'] = hist_DY\n",
    "    \n",
    "    #sum background\n",
    "    hist_b = sum_hists([hist_proc_dict[proc] for proc in b_proc_list])\n",
    "    hist_proc_dict['b'] = hist_b\n",
    "    \n",
    "    #sum background + signal\n",
    "    hist_bs = sum_hists([hist_proc_dict[proc] for proc in bs_proc_list])\n",
    "    hist_proc_dict['bs'] = hist_bs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% reshape hist dicts\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# amp = trans_data / trans_mc = (n_data_a / n_data) / (n_mc_a / n_mc)\n",
    "\n",
    "hist_mc_b = hist_cut_proc_dict['hist_ano']['b'] # background (W, DY)\n",
    "hist_mc_bs = hist_cut_proc_dict['hist_ano']['bs'] # background + ttbar\n",
    "\n",
    "def compute_amp(hist_pre, hist_ano):\n",
    "    hist_amp = {}\n",
    "\n",
    "    for key in hist_pre.keys():\n",
    "        hist_amp[key] = Histogram(hist_pre[key].bin_edges)\n",
    "        hist_amp[key].bin_content = hist_ano[key].bin_content / (hist_pre[key].bin_content+1)\n",
    "    \n",
    "    return hist_amp\n",
    "\n",
    "# hist_amp_dict[proc]\n",
    "hist_amp_dict = {}\n",
    "hist_amp_dict['b'] = compute_amp(hist_cut_proc_dict['hist_pre']['b'], hist_cut_proc_dict['hist_ano']['b'])\n",
    "hist_amp_dict['bs'] = compute_amp(hist_cut_proc_dict['hist_pre']['bs'], hist_cut_proc_dict['hist_ano']['bs'])\n",
    "hist_amp_dict['data'] = compute_amp(hist_cut_proc_dict['hist_pre']['data'], hist_cut_proc_dict['hist_ano']['data'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% compute ADSE (anomaly detector selection efficiency)\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "feature_list = ['HT', 'mass_jet', 'n_jet', 'n_bjet']\n",
    "background_plot_list = ['W', 'DY', 'ttbar']\n",
    "\n",
    "plot_size = 5\n",
    "\n",
    "color_dict = {}\n",
    "color_dict['W']='tab:blue'\n",
    "color_dict['DY']='tab:green'\n",
    "color_dict['ttbar']='tab:red'\n",
    "color_dict['data']='tab:purple'\n",
    "\n",
    "label_dict = {}\n",
    "label_dict['W'] = r'$W \\rightarrow \\mu\\nu$'\n",
    "label_dict['DY'] = r'$Z/\\gamma^* \\rightarrow ll$'\n",
    "label_dict['ttbar'] = r'$t\\bar{t}$'\n",
    "\n",
    "all_lin = False\n",
    "\n",
    "n_features = len(feature_list)\n",
    "fig, ax_arr = plt.subplots(n_features, 4, figsize=(4 * plot_size, n_features * plot_size))\n",
    "\n",
    "for row, feature in enumerate(feature_list):\n",
    "    f_setting = hist_settings[feature]\n",
    "    \n",
    "    # col 0, 1\n",
    "    for col, cut in enumerate(['hist_pre', 'hist_ano']):\n",
    "        ax = ax_arr[row, col]\n",
    "    \n",
    "        # plot background stacked\n",
    "        x_list = []\n",
    "        bin_edges = None\n",
    "        weights_list = []\n",
    "        color_list = []\n",
    "        label_list = []\n",
    "        \n",
    "        for proc in background_plot_list:\n",
    "            hist = hist_cut_proc_dict[cut][proc]\n",
    "            bin_edges = hist[feature].bin_edges\n",
    "            x_list.append(hist[feature].bin_edges[:-1])\n",
    "            weights_list.append(hist[feature].bin_content)\n",
    "            color_list.append(color_dict[proc])\n",
    "            label_list.append(label_dict[proc])\n",
    "            \n",
    "        ax.hist(x_list, bin_edges, weights=weights_list, \n",
    "                stacked=True, histtype='stepfilled', color=color_list, label=label_list)\n",
    "        \n",
    "        # axis settings\n",
    "        if all_lin is False:\n",
    "            ax.set_yscale(f_setting['yscale'])\n",
    "            \n",
    "        ax.set_xlabel(f_setting['symbol'])\n",
    "\n",
    "    # col 2\n",
    "    ax = ax_arr[row, 2]\n",
    "    \n",
    "    # plot data\n",
    "    hist_data_f = hist_cut_proc_dict['hist_pre']['data'][feature]\n",
    "    bin_edges = hist_data_f.bin_edges\n",
    "    bin_content = hist_data_f.bin_content\n",
    "    ax.hist(bin_edges[:-1], bin_edges, weights=bin_content, histtype='stepfilled', density=True,color=color_dict['data'], label='data')\n",
    "    \n",
    "    # plot data anomalous\n",
    "    hist_data_f = hist_cut_proc_dict['hist_ano']['data'][feature]\n",
    "    bin_edges = hist_data_f.bin_edges\n",
    "    bin_content = hist_data_f.bin_content\n",
    "    ax.hist(bin_edges[:-1], bin_edges, weights=bin_content, histtype='step', color='b', label='data a', density=True)\n",
    "\n",
    "    # axis settings\n",
    "    if all_lin is False:\n",
    "        ax.set_yscale(f_setting['yscale'])\n",
    "\n",
    "# fix ppm\n",
    "# uncertainty (opt)\n",
    "\n",
    "# plot ADSE\n",
    "for row, feature in enumerate(feature_list):\n",
    "    ax = ax_arr[row, 3]\n",
    "    f_setting = hist_settings[feature]\n",
    "    \n",
    "    bin_edges = hist_amp_dict['b'][feature].bin_edges\n",
    "    bin_content = hist_amp_dict['b'][feature].bin_content\n",
    "    ax.hist(bin_edges[:-1], bin_edges, weights = bin_content, histtype='stepfilled',# density=True,\n",
    "            label='background', color='blue')\n",
    "\n",
    "    bin_edges = hist_amp_dict['bs'][feature].bin_edges\n",
    "    bin_content = hist_amp_dict['bs'][feature].bin_content\n",
    "    ax.hist(bin_edges[:-1], bin_edges, weights = bin_content, histtype='step',# density=True,\n",
    "            label='W+DY+tt', color='red')\n",
    "    \n",
    "    bin_edges = hist_amp_dict['data'][feature].bin_edges\n",
    "    bin_content = hist_amp_dict['data'][feature].bin_content\n",
    "    ax.hist(bin_edges[:-1], bin_edges, weights = bin_content, histtype='step',# density=True,\n",
    "            label='W+DY+tt', color='magenta')\n",
    "\n",
    "    # bin_edges = hist_amp_dict['data'][feature].bin_edges\n",
    "    # bin_centers = (bin_edges[:-1]+bin_edges[1:])/2\n",
    "    # bin_content = hist_amp_dict['data'][feature].bin_content\n",
    "    # ax.plot(bin_centers, bin_content, 'bo', label='data')\n",
    "    \n",
    "    ax.set_yscale('log')\n",
    "\n",
    "handles, labels = ax_arr[0,0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc = 'upper center', ncol=4)\n",
    "# fig.legend(handles, labels, ncol=4, bbox_to_anchor=(0., 1.02, 1., .102), loc='lower left',\n",
    "#         mode=\"expand\", borderaxespad=0.)\n",
    "\n",
    "\n",
    "plt.savefig('figures/adse.pdf')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% # ms stack vs data (after pre), ...(after ano), (amp)\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#plots\n",
    "# mc detailed no stack\n",
    "\n",
    "# data vs mc (after cuts)\n",
    "\n",
    "# data_a vs mc_a (with ppm fixed) (with ad trained on clean mc)\n",
    "\n",
    "# amplification \n",
    "\n",
    "# normal and anomalous b=0,1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% build postselection hists\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax_arr = plt.subplots(n_features, 1, figsize=(1 * plot_size, n_features * plot_size))\n",
    "\n",
    "for row, feature in enumerate(feature_list):\n",
    "    f_setting = hist_settings[feature]\n",
    " \n",
    "\n",
    "    ax = ax_arr[row]\n",
    "    \n",
    "    # plot background stacked\n",
    "    x_list = []\n",
    "    bin_edges = None\n",
    "    weights_list = []\n",
    "    color_list = []\n",
    "    label_list = []\n",
    "    \n",
    "    for proc in background_plot_list:\n",
    "        hist = hist_cut_proc_dict['hist_pos'][proc]\n",
    "        bin_edges = hist[feature].bin_edges\n",
    "        x_list.append(hist[feature].bin_edges[:-1])\n",
    "        weights_list.append(hist[feature].bin_content)\n",
    "        color_list.append(color_dict[proc])\n",
    "        label_list.append(label_dict[proc])\n",
    "        \n",
    "    ax.hist(x_list, bin_edges, weights=weights_list, \n",
    "            stacked=True, histtype='stepfilled', color=color_list, label=label_list, density=True)\n",
    "    \n",
    "    hist_data_f = hist_cut_proc_dict['hist_pos']['data'][feature]\n",
    "    bin_edges = hist_data_f.bin_edges\n",
    "    bin_content = hist_data_f.bin_content\n",
    "    ax.hist(bin_edges[:-1], bin_edges, weights=bin_content, histtype='step', density=True, color=color_dict['data'], label='data')\n",
    "    \n",
    "    \n",
    "    # axis settings\n",
    "    if all_lin is False:\n",
    "        ax.set_yscale(f_setting['yscale'])\n",
    "        \n",
    "    ax.set_xlabel(f_setting['symbol'])\n",
    "\n",
    "    \n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}