{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import importlib.util\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from alad_mod.alad import ALAD\n",
    "from evaluation.histogram_builder import *\n",
    "from data.hlf_preprocessing import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false,
     "name": "#%% Load ALAD\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "loading alad\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\nW1026 09:13:15.526809 139750867810112 lazy_loader.py:50] \nThe TensorFlow contrib module will not be included in TensorFlow 2.0.\nFor more information, please see:\n  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n  * https://github.com/tensorflow/addons\n  * https://github.com/tensorflow/io (for I/O related ops)\nIf you depend on functionality not listed there, please file an issue.\n\n",
      "W1026 09:13:15.600625 139750867810112 deprecation_wrapper.py:119] From /home/oliverkn/cloud/eth/2019_FS/pro/pycharm/alad_mod/alad.py:29: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n\n",
      "W1026 09:13:15.603920 139750867810112 deprecation_wrapper.py:119] From /home/oliverkn/cloud/eth/2019_FS/pro/pycharm/alad_mod/alad.py:44: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n\n",
      "W1026 09:13:15.605090 139750867810112 deprecation.py:323] From /home/oliverkn/pro/alad_6021/2_l16/config.py:81: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse keras.layers.dense instead.\n",
      "W1026 09:13:16.023033 139750867810112 deprecation.py:323] From /home/oliverkn/pro/alad_6021/2_l16/config.py:168: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
      "W1026 09:13:16.141122 139750867810112 deprecation.py:323] From /home/oliverkn/pro/alad_6021/2_l16/config.py:176: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse keras.layers.dropout instead.\n",
      "W1026 09:13:16.489027 139750867810112 deprecation.py:323] From /home/oliverkn/pro/pycharm/venv/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W1026 09:13:16.576167 139750867810112 deprecation_wrapper.py:119] From /home/oliverkn/cloud/eth/2019_FS/pro/pycharm/alad_mod/alad.py:126: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n\n",
      "W1026 09:13:16.577033 139750867810112 deprecation_wrapper.py:119] From /home/oliverkn/cloud/eth/2019_FS/pro/pycharm/alad_mod/alad.py:133: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n\n",
      "W1026 09:13:16.577526 139750867810112 deprecation_wrapper.py:119] From /home/oliverkn/cloud/eth/2019_FS/pro/pycharm/alad_mod/alad.py:133: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n\n",
      "W1026 09:13:16.578029 139750867810112 deprecation_wrapper.py:119] From /home/oliverkn/cloud/eth/2019_FS/pro/pycharm/alad_mod/alad.py:143: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n\n",
      "W1026 09:13:18.839407 139750867810112 deprecation.py:323] From /home/oliverkn/pro/pycharm/venv/lib/python3.6/site-packages/tensorflow/python/training/moving_averages.py:433: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "W1026 09:13:19.660118 139750867810112 deprecation.py:323] From /home/oliverkn/pro/pycharm/venv/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse keras.layers.flatten instead.\n",
      "W1026 09:13:19.863035 139750867810112 deprecation.py:506] From /home/oliverkn/cloud/eth/2019_FS/pro/pycharm/alad_mod/alad.py:214: calling norm (from tensorflow.python.ops.linalg_ops) with keep_dims is deprecated and will be removed in a future version.\nInstructions for updating:\nkeep_dims is deprecated, use keepdims instead\n",
      "W1026 09:13:19.898127 139750867810112 deprecation_wrapper.py:119] From /home/oliverkn/cloud/eth/2019_FS/pro/pycharm/alad_mod/alad.py:429: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n\n",
      "W1026 09:13:20.006464 139750867810112 deprecation.py:323] From /home/oliverkn/pro/pycharm/venv/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse standard file APIs to check for files with this prefix.\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "result_path = '/home/oliverkn/pro/alad_6021/2_l16'\n",
    "model_file = 'model-10000000'\n",
    "\n",
    "print('loading alad')\n",
    "\n",
    "# loading config\n",
    "spec = importlib.util.spec_from_file_location('config', os.path.join(result_path, 'config.py'))\n",
    "config_alad = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(config_alad)\n",
    "\n",
    "# loading preprocessor\n",
    "preprocessor = load(os.path.join(result_path, 'preprocessor.pkl'))\n",
    "\n",
    "# loading alad\n",
    "tf.reset_default_graph()\n",
    "ad = ALAD(config_alad, tf.Session())\n",
    "ad.load(os.path.join(result_path, model_file))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "thres = 110\n",
    "score_type = 'fm'\n",
    "\n",
    "cont_bins = 20\n",
    "settings_6021 = {}\n",
    "settings_6021['HT'] = {'range': (0, 4000), 'yscale': 'log', 'bins': cont_bins, 'int': False}\n",
    "settings_6021['mass_jet'] = {'range': (0, 4000), 'yscale': 'log', 'bins': cont_bins, 'int': False}\n",
    "settings_6021['n_jet'] = {'range': (0, 15), 'yscale': 'linear', 'int': True, 'bin_size': 1}\n",
    "settings_6021['n_bjet'] = {'range': (0, 15), 'yscale': 'linear', 'int': True, 'bin_size': 1}\n",
    "settings_6021['lep_pt'] = {'range': (20, 1000), 'yscale': 'log', 'bins': cont_bins, 'int': False}\n",
    "settings_6021['lep_eta'] = {'range': (-2.5, 2.5), 'yscale': 'linear', 'bins': cont_bins, 'int': False}\n",
    "settings_6021['lep_charge'] = {'range': (-1, 1), 'yscale': 'linear', 'int': True}\n",
    "settings_6021['lep_iso_ch'] = {'range': (0, 0.4), 'yscale': 'log', 'bins': cont_bins, 'int': False}\n",
    "settings_6021['lep_iso_neu'] = {'range': (0, 0.4), 'yscale': 'log', 'bins': cont_bins, 'int': False}\n",
    "settings_6021['lep_iso_gamma'] = {'range': (0, 0.4), 'yscale': 'log', 'bins': cont_bins, 'int': False}\n",
    "settings_6021['MET'] = {'range': (0, 1000), 'yscale': 'log', 'bins': cont_bins, 'int': False}\n",
    "settings_6021['METo'] = {'range': (-100, 100), 'yscale': 'linear', 'bins': cont_bins, 'int': False}\n",
    "settings_6021['METp'] = {'range': (-100, 100), 'yscale': 'linear', 'bins': cont_bins, 'int': False}\n",
    "settings_6021['MT'] = {'range': (0, 200), 'yscale': 'log', 'bins': cont_bins, 'int': False}\n",
    "settings_6021['n_mu'] = {'range': (0, 15), 'yscale': 'linear', 'int': True, 'bin_size': 1}\n",
    "settings_6021['pt_mu'] = {'range': (0, 1000), 'yscale': 'log', 'bins': cont_bins, 'int': False}\n",
    "settings_6021['mass_mu'] = {'range': (0, 1000), 'yscale': 'log', 'bins': cont_bins, 'int': False}\n",
    "settings_6021['n_ele'] = {'range': (0, 15), 'yscale': 'linear', 'int': True, 'bin_size': 1}\n",
    "settings_6021['pt_ele'] = {'range': (0, 1000), 'yscale': 'log', 'bins': cont_bins, 'int': False}\n",
    "settings_6021['mass_ele'] = {'range': (0, 1000), 'yscale': 'log', 'bins': cont_bins, 'int': False}\n",
    "settings_6021['n_neu'] = {'range': (0, 400), 'yscale': 'linear', 'int': True, 'bin_size': 1}\n",
    "settings_6021['n_ch'] = {'range': (0, 1000), 'yscale': 'linear', 'int': True, 'bin_size': 1}\n",
    "settings_6021['n_photon'] = {'range': (0, 1000), 'yscale': 'linear', 'int': True, 'bin_size': 1}\n",
    "hist_settings = settings_6021\n",
    "\n",
    "def pre_select(x):\n",
    "    filter_eta = np.abs(x[:,5]) < 1\n",
    "    filter_njets = x[:,2] > 1\n",
    "\n",
    "    filter_idx = filter_njets * filter_eta \n",
    "    \n",
    "    return x[filter_idx]\n",
    "\n",
    "def anomaly_select(x):\n",
    "    x_transformed = preprocessor.transform(x)\n",
    "    scores = ad.get_anomaly_scores(x_transformed, type=score_type)\n",
    "    anomaly_idx = scores > thres\n",
    "    return x[anomaly_idx]\n",
    "\n",
    "def post_select(x):\n",
    "    filter_bjets =  x[:,3] > 1\n",
    "    filter_idx = filter_bjets \n",
    "    \n",
    "    return x[filter_idx]\n",
    "\n",
    "def build_hists(x, n_max, batch_size=2 ** 20):\n",
    "    builder_raw = HistogramBuilder(hist_settings)\n",
    "    builder_pre = HistogramBuilder(hist_settings)\n",
    "    builder_ano= HistogramBuilder(hist_settings)\n",
    "    builder_pos = HistogramBuilder(hist_settings)\n",
    "\n",
    "    n = x.shape[0]\n",
    "    n = min(n, n_max)\n",
    "    #sample_fraction = min(n_max/n, 1)\n",
    "    #sample_size = int(batch_size*sample_fraction)\n",
    "\n",
    "    n_batches = int(n / batch_size) + 1\n",
    "    for t in range(n_batches):\n",
    "        print('batch number ' + str(t))\n",
    "        ran_from = int(t * batch_size)\n",
    "        ran_to = (t + 1) * batch_size\n",
    "        ran_to = int(np.clip(ran_to, 0, n))\n",
    "        x_raw = x[ran_from:ran_to]\n",
    "        \n",
    "        #if sample_fraction < 0.5:\n",
    "        #    idx = np.arange(0,batch_size)\n",
    "        #    idx = np.random.choice(idx, sample_size, replace=False)\n",
    "        #    x_raw = x_batch[idx]\n",
    "        #else:\n",
    "        #    x_raw = x_batch\n",
    "        \n",
    "        \n",
    "        # raw\n",
    "        builder_raw.add_data(x_raw)\n",
    "        \n",
    "        # pre selection\n",
    "        x_pre = pre_select(x_raw)\n",
    "        builder_pre.add_data(x_pre)\n",
    "        \n",
    "        # anomaly selection\n",
    "        x_ano = anomaly_select(x_pre)\n",
    "        builder_ano.add_data(x_ano)\n",
    "        \n",
    "        # post selection\n",
    "        x_pos = post_select(x_ano)\n",
    "        builder_pos.add_data(x_pos)\n",
    "        \n",
    "    return builder_raw.get_histogram_data(), builder_pre.get_histogram_data(),\\\n",
    "           builder_ano.get_histogram_data(), builder_pos.get_histogram_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "------------------------------building dy1jets\nbatch number 0\n",
      "151 / 1000000 = 151_ppm\nprocessed/available: 1000000/18911952 = 0.052877\nN_tot=65949083, N_target=3056142, w=0.876398\n------------------------------building dy2jets\nbatch number 0\n",
      "265 / 1000000 = 265_ppm\nprocessed/available: 1000000/6491172 = 0.154055\nN_tot=21732511, N_target=986028, w=0.294512\n------------------------------building dy3jets\nbatch number 0\n",
      "721 / 1000000 = 721_ppm\nprocessed/available: 1000000/3401866 = 0.293956\nN_tot=11025803, N_target=277831, w=0.085721\n------------------------------building dy4jets\nbatch number 0\n",
      "2884 / 1000000 = 2884_ppm\nprocessed/available: 1000000/2018671 = 0.495375\nN_tot=6361350, N_target=81715, w=0.025931\n------------------------------building w1jets\nbatch number 0\n",
      "153 / 1000000 = 153_ppm\nprocessed/available: 1000000/6009999 = 0.166389\nN_tot=29784800, N_target=24405561, w=4.924572\n------------------------------building w2jets\nbatch number 0\n",
      "209 / 1000000 = 209_ppm\nprocessed/available: 1000000/6452984 = 0.154967\nN_tot=30693853, N_target=7817406, w=1.643508\n------------------------------building w3jets\nbatch number 0\n",
      "333 / 1000000 = 333_ppm\nprocessed/available: 1000000/3256494 = 0.307079\nN_tot=15241144, N_target=1656091, w=0.353848\n------------------------------building ttbar\nbatch number 0\n",
      "3727 / 1000000 = 3727_ppm\nprocessed/available: 1000000/10757944 = 0.092955\nN_tot=62131965, N_target=1205750, w=0.208772\n------------------------------building data\nbatch number 0\n",
      "105 / 1000000 = 105_ppm\nprocessed/available: 1000000/38247438 = 0.026146\nN_tot=4429, N_target=4429, w=38.247438\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "#compute hist for raw, pres, ad, pos\n",
    "\n",
    "n_max = int(1e6)\n",
    "\n",
    "# data\n",
    "lum = 4429.0\n",
    "#lum = 3325.3 # adjusted\n",
    "\n",
    "sets = {}\n",
    "sets['dy1jets'] = {'file': '/home/oliverkn/pro/7719/data.hdf5', 'xsec':561, 'K':1.23, 'N_tot':65949083}\n",
    "sets['dy2jets'] = {'file': '/home/oliverkn/pro/7721/data.hdf5', 'xsec':181, 'K':1.23, 'N_tot':21732511}\n",
    "sets['dy3jets'] = {'file': '/home/oliverkn/pro/7722/data.hdf5', 'xsec':51, 'K':1.23, 'N_tot':11025803}\n",
    "sets['dy4jets'] = {'file': '/home/oliverkn/pro/7723/data.hdf5', 'xsec':15, 'K':1.23, 'N_tot':6361350}\n",
    "\n",
    "sets['w1jets'] = {'file': '/home/oliverkn/pro/9863/data.hdf5', 'xsec':4480, 'K':1.23, 'N_tot':29784800}\n",
    "sets['w2jets'] = {'file': '/home/oliverkn/pro/9864/data.hdf5', 'xsec':1435, 'K':1.23, 'N_tot':30693853}\n",
    "sets['w3jets'] = {'file': '/home/oliverkn/pro/9865/data.hdf5', 'xsec':304, 'K':1.23, 'N_tot':15241144}\n",
    "\n",
    "sets['ttbar'] = {'file': '/home/oliverkn/pro/9588/data.hdf5', 'xsec':164, 'K':1.66, 'N_tot':62131965}\n",
    "\n",
    "sets['data'] = {'file': '/home/oliverkn/pro/6021/data.hdf5', 'xsec':1, 'K':1, 'N_tot':lum}\n",
    "\n",
    "\n",
    "for key, set in sets.items():\n",
    "    print('------------------------------building ' + key)\n",
    "    \n",
    "    # load data\n",
    "    file = set['file']\n",
    "    x = h5py.File(file, 'r')['data']\n",
    "    \n",
    "    # build hists\n",
    "    hist_raw, hist_pre, hist_ano, hist_pos = build_hists(x, n_max=n_max)\n",
    "    \n",
    "    n_events = hist_raw['HT'].n\n",
    "    n_events_a = hist_ano['HT'].n\n",
    "    print('%d / %d = %3d_ppm' % (n_events_a, n_events, (n_events_a / n_events*1e6)))\n",
    "    \n",
    "    # weight\n",
    "    fraction_processed = n_events/x.shape[0]\n",
    "    print('processed/available: %d/%d = %f'%(n_events, x.shape[0],fraction_processed))\n",
    "    \n",
    "    N_tot = set['N_tot']\n",
    "    N_target = lum * set['K'] * set['xsec']\n",
    "    weight =  N_target / (N_tot * fraction_processed)\n",
    "    print('N_tot=%d, N_target=%d, w=%f'%(N_tot, N_target, weight))\n",
    "    \n",
    "    # scaling up hists\n",
    "    hist_raw = scale_hists(hist_raw, weight)\n",
    "    hist_pre = scale_hists(hist_pre, weight)\n",
    "    hist_ano = scale_hists(hist_ano, weight)\n",
    "    hist_pos = scale_hists(hist_pos, weight)\n",
    "    \n",
    "    set['hist_raw'] = hist_raw\n",
    "    set['hist_pre'] = hist_pre\n",
    "    set['hist_ano'] = hist_ano\n",
    "    set['hist_pos'] = hist_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "w1jets: 4924572 (0.585334)\nw2jets: 1643508 (0.195347)\nw3jets: 353848 (0.042058)\ndy1jets: 876397 (0.104169)\ndy2jets: 294511 (0.035006)\ndy3jets: 85721 (0.010189)\ndy4jets: 25930 (0.003082)\nttbar: 208771 (0.024815)\ntotal number of events: 8413262\ntotal number of events: 38247437\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "sum_n = 0\n",
    "for key in ['w1jets', 'w2jets', 'w3jets', 'dy1jets', 'dy2jets', 'dy3jets', 'dy4jets', 'ttbar']:\n",
    "    n = sets[key]['hist_raw']['HT'].n\n",
    "    sum_n += n\n",
    "\n",
    "for key in ['w1jets', 'w2jets', 'w3jets', 'dy1jets', 'dy2jets', 'dy3jets', 'dy4jets', 'ttbar']:\n",
    "    n = sets[key]['hist_raw']['HT'].n\n",
    "    print('%s: %d (%f)'%(key, n, n/sum_n))\n",
    "    \n",
    "print('total number of events: %d' % sum_n)\n",
    "print('total number of events: %d' % sets['data']['hist_raw']['HT'].n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "hist_type_dict = {}\n",
    "for hist_type in ['hist_raw', 'hist_pre', 'hist_ano','hist_pos']:\n",
    "    hist_proc_dict = {}\n",
    "    for proc in ['w1jets', 'w2jets', 'w3jets', 'dy1jets', 'dy2jets', 'dy3jets', 'dy4jets', 'ttbar']:\n",
    "        #hist_proc_scaled = scale_hists(sets[proc][hist_type], sets[proc]['weight'])\n",
    "        hist_proc = sets[proc][hist_type]\n",
    "        hist_proc_dict[proc] = hist_proc\n",
    "    hist_type_dict[hist_type]=hist_proc_dict\n",
    "\n",
    "hist_type_dict_compressed = {}\n",
    "for hist_type in ['hist_raw', 'hist_pre', 'hist_ano','hist_pos']:\n",
    "    hist_proc = hist_type_dict[hist_type]\n",
    "    hist_proc_dict = {}\n",
    "    \n",
    "    # sum W\n",
    "    hist_w_list = [hist_proc[proc] for proc in ['w1jets', 'w2jets', 'w3jets']]\n",
    "    hist_w = sum_hists(hist_w_list, [1,1,1])\n",
    "    hist_proc_dict['W'] = hist_w\n",
    "    \n",
    "    #sum DY\n",
    "    hist_dy_list = [hist_proc[proc] for proc in ['dy1jets', 'dy2jets', 'dy3jets', 'dy4jets']]\n",
    "    hist_dy = sum_hists(hist_dy_list, [1,1,1,1])\n",
    "    hist_proc_dict['DY'] = hist_dy\n",
    "    \n",
    "    # ttbar\n",
    "    hist_proc_dict['tt'] = hist_proc['ttbar']\n",
    "    \n",
    "    # add to type dict\n",
    "    hist_type_dict_compressed[hist_type]=hist_proc_dict\n",
    "\n",
    "data_hist_type_dict={}\n",
    "for hist_type in ['hist_raw', 'hist_pre', 'hist_ano','hist_pos']:\n",
    "    data_hist_type_dict[hist_type] = sets['data'][hist_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "---------------------hist_raw---------------------\nhist_raw/W total events: 6921928\nhist_raw/DY total events: 1282561\nhist_raw/tt total events: 208771\nn_data/n_mc: 38247437/8413262 = 4.546089\n---------------------hist_pre---------------------\nhist_pre/W total events: 2365332\nhist_pre/DY total events: 501191\nhist_pre/tt total events: 125623\nn_data/n_mc: 9674957/2992146 = 3.233450\n---------------------hist_ano---------------------\nhist_ano/W total events: 1214\nhist_ano/DY total events: 346\nhist_ano/tt total events: 778\nn_data/n_mc: 4015/2339 = 1.716343\n---------------------hist_pos---------------------\nhist_pos/W total events: 101\nhist_pos/DY total events: 12\nhist_pos/tt total events: 371\nn_data/n_mc: 535/486 = 1.101554\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "#print event numbers\n",
    "for hist_type in ['hist_raw', 'hist_pre', 'hist_ano','hist_pos']:\n",
    "    print('---------------------%s---------------------' % hist_type)\n",
    "    n_mc = 0\n",
    "    for proc in ['W', 'DY', 'tt']:\n",
    "        n = hist_type_dict_compressed[hist_type][proc]['HT'].n\n",
    "        n_mc += n\n",
    "        print('%s/%s total events: %d' %(hist_type, proc, n))\n",
    "    \n",
    "    n_data = data_hist_type_dict[hist_type]['HT'].n\n",
    "    print('n_data/n_mc: %d/%d = %f'%(n_data, n_mc, n_data/n_mc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "saving fig to hists.pdf\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "#hist_data_list[hist_type][proc][feature]\n",
    "#hist_data_type_dict[hist_type][feature]\n",
    "def plot_hist(hist_data_list, hist_data_type_dict, settings, output_file=None, all_lin=False):\n",
    "    f, ax_arr = plt.subplots(23, len(hist_data_list.keys()), figsize=(18, 100))\n",
    "\n",
    "    for col, hist_type in enumerate(hist_data_list.keys()):\n",
    "    \n",
    "        for row, feature in enumerate(settings.keys()):\n",
    "            ax = ax_arr[row, col]\n",
    "            fsettings = settings[feature]\n",
    "\n",
    "            x_list = []\n",
    "            bin_edges = None\n",
    "            weights_list = []\n",
    "            \n",
    "            for proc, hist in hist_data_list[hist_type].items():\n",
    "                bin_edges = hist[feature].bin_edges\n",
    "                x_list.append(hist[feature].bin_edges[:-1])\n",
    "                weights_list.append(hist[feature].bin_content)\n",
    "            \n",
    "            ax.hist(x_list, bin_edges, weights=weights_list,stacked=True, label=hist_data_list[hist_type].keys(), histtype='stepfilled')\n",
    "\n",
    "            # plot data\n",
    "            hist_data_f = hist_data_type_dict[hist_type][feature]\n",
    "            bin_centers = (hist_data_f.bin_edges[:-1]+hist_data_f.bin_edges[1:])/2\n",
    "            ax.errorbar(bin_centers, hist_data_f.bin_content, yerr=np.sqrt(hist_data_f.bin_content), fmt='bo', label='data')\n",
    "            \n",
    "            if all_lin is False:\n",
    "                ax.set_yscale(fsettings['yscale'])\n",
    "                \n",
    "            ax.set_title(feature)\n",
    "            ax.legend()\n",
    "\n",
    "    if output_file is not None:\n",
    "        print('saving fig to ' + output_file)\n",
    "        plt.savefig(output_file)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "plot_hist(hist_type_dict_compressed,data_hist_type_dict, hist_settings)#, output_file='hists.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(6,6))\n",
    "\n",
    "w = np.array([10,15,12,8,4])*100\n",
    "tt = np.array([0, 1, 5, 1, 1])*100\n",
    "data = w + tt\n",
    "\n",
    "bin_edges = np.arange(0, 6)-0.5\n",
    "\n",
    "bin_centers = (bin_edges[:-1] + bin_edges[1:])/2\n",
    "\n",
    "ax.hist([bin_edges[:-1]]*2, bin_edges, weights=[w, tt], label=['W', 'tt'],stacked=True, histtype='stepfilled')\n",
    "ax.errorbar(bin_centers, data, yerr=np.sqrt(data), fmt='bo', label='data')\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x = h5py.File('/home/oliverkn/pro/7719/data.hdf5', 'r')['data']\n",
    "\n",
    "x_batch = x[:1000000]\n",
    "\n",
    "a = np.arange(0,1000000)\n",
    "s = np.random.choice(a, 10000, replace=False)\n",
    "print(x_batch[s].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bin_edges = np.arange(0, 1 + 2) - 0.5\n",
    "print(bin_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}